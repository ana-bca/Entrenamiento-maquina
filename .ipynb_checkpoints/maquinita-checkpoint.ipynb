{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from skimage import io\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from spacy.lang.en import English\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer()\n",
    "nlp = English()\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import regularizers, optimizers\n",
    "from skimage.transform import rescale, resize, downscale_local_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q xlrd\n",
    "!git clone https://github.com/ana-bca/Entrenamiento-maquina"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_photo_F = pd.read_csv('Entrenamiento-maquina/Datos_recortados.csv', engine='python',sep=\",\")\n",
    "data_train = pd.read_csv('Entrenamiento-maquina/Imagenes1/data_train.csv', engine='python',sep=\"\\t\")\n",
    "data_test = pd.read_csv('Entrenamiento-maquina/Imagenes1/data_test.csv', engine='python',sep=\"\\t\")\n",
    "data_valid = pd.read_csv('Entrenamiento-maquina/Imagenes1/data_valid.csv', engine='python',sep=\"\\t\")\n",
    "\n",
    "Ngrams_1 = pd.read_csv('Entrenamiento-maquina/palabra30.csv', engine='python',sep=\"\\t\")\n",
    "#Ngrams_2 = pd.read_csv('prueba1.csv',sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones ==================================================================#\n",
    "#N2 = Ngrams_2.compuesta.values\n",
    "def TUPT(x):\n",
    "    x_N1 = np.array(NGRAM(x,1))\n",
    "    x_N1 = x_N1.reshape(x_N1.shape[0],)\n",
    "    #x_N2 = pd.DataFrame(np.array(NGRAM(data_F.caption[0],2)))\n",
    "    #x_N2 = x_N2.agg(' '.join, axis=1).values\n",
    "    #x_N2 = np.array([x for x in x_N2 if x in N2])\n",
    "    #x_T = list(np.append(x_N1.astype(str), x_N2.astype(str)))\n",
    "    x_T = list(set(x_N1.astype(str)))\n",
    "    return x_T\n",
    "def NGRAM(x,n):\n",
    "    token=nltk.word_tokenize(str(x))\n",
    "    bigrams=ngrams(token,n)\n",
    "    return list(bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codigo =====================================================================#\n",
    "data_photo_F = shuffle(data_photo_F)\n",
    "data_photo_F = data_photo_F[['photo_id','caption',]]\n",
    "data_photo_F.photo_id = data_photo_F.photo_id  + '.jpg'\n",
    "# Separacion de las imagenes en entrenamiento, test y validacion\n",
    "data_train, data_test = train_test_split(data_photo_F,test_size=0.2,random_state=1000)\n",
    "data_train, data_valid = train_test_split(data_train,test_size=20/80,random_state=1000)\n",
    "csv_train = \"Entrenamiento-maquina/Imagenes1/data_train.csv\"\n",
    "csv_test = \"Entrenamiento-maquina/Imagenes1/data_test.csv\"\n",
    "csv_valid = \"Entrenamiento-maquina/Imagenes1/data_valid.csv\"\n",
    "data_train.to_csv(csv_train , index=False,sep=\"\\t\")\n",
    "data_test.to_csv(csv_test , index=False,sep=\"\\t\")\n",
    "data_valid.to_csv(csv_valid , index=False,sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm.tqdm(data_test.photo_id.values):\n",
    "    Imagen = io.imread('photos/' + i)\n",
    "    image_resized = resize(Imagen, (400,500),anti_aliasing=True)\n",
    "    local = 'Entrenamiento-maquina/Imagenes1/test/' + i\n",
    "    io.imsave(local,image_resized)\n",
    "for i in tqdm.tqdm(data_train.photo_id.values):\n",
    "    Imagen = io.imread('photos/' + i)\n",
    "    image_resized = resize(Imagen, (400,500),anti_aliasing=True)\n",
    "    local = 'Entrenamiento-maquina/Imagenes1/train/' + i\n",
    "    io.imsave(local,image_resized)\n",
    "for i in tqdm.tqdm(data_valid.photo_id.values):\n",
    "    Imagen = io.imread('photos/' + i)\n",
    "    image_resized = resize(Imagen, (400,500),anti_aliasing=True)\n",
    "    local = 'Entrenamiento-maquina/Imagenes1/valid/' + i\n",
    "    io.imsave(local,image_resized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " data_train, data_test = train_test_split(data_photo_F,test_size=0.2,random_state=1000)\n",
    "data_train, data_valid = train_test_split(data_train,test_size=20/80,random_state=1000)\n",
    "csv_train = \"Entrenamiento-maquina/Imagenes1/data_train.csv\"\n",
    "csv_test = \"Entrenamiento-maquina/Imagenes1/data_test.csv\"\n",
    "csv_valid = \"Entrenamiento-maquina/Imagenes1/data_valid.csv\"\n",
    "data_train.to_csv(csv_train , index=False,sep=\"\\t\")\n",
    "data_test.to_csv(csv_test , index=False,sep=\"\\t\")\n",
    "data_valid.to_csv(csv_valid , index=False,sep=\"\\t\")\n",
    "nltk.download('punkt')\n",
    "data_train = data_train.assign(tags = np.array(data_train.caption.map(lambda p: TUPT(p))))\n",
    "data_test = data_test.assign(tags = np.array(data_test.caption.map(lambda p: TUPT(p))))\n",
    "data_valid = data_valid.assign(tags = np.array(data_valid.caption.map(lambda p: TUPT(p))))\n",
    "# Cree un ImageDataGenerator con flow_from_dataframe\n",
    "## Clases en los datos\n",
    "Ngrams_1 = Ngrams_1.word.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = list(set(Ngrams_1.astype(str)))\n",
    "datagen_train = ImageDataGenerator(\n",
    "    rescale = 1./255,\n",
    "    zoom_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    rotation_range = 5,\n",
    "    horizontal_flip=True)\n",
    "datagen = ImageDataGenerator(rescale = 1./255)\n",
    "train_generator = datagen_train.flow_from_dataframe(\n",
    "    dataframe = data_train[[\"photo_id\",\"tags\"]].sample(5000),\n",
    "    directory = \"Entrenamiento-maquina//Imagenes1/train\",\n",
    "    x_col = \"photo_id\",\n",
    "    y_col = \"tags\",\n",
    "    batch_size = 32,\n",
    "    class_mode = \"categorical\",\n",
    "    shuffle=True,\n",
    "    color_mode = \"rgb\",\n",
    "    classes = keys,\n",
    "    target_size = (400,500))\n",
    "\n",
    "valid_generator = datagen.flow_from_dataframe(\n",
    "    dataframe = data_valid[[\"photo_id\",\"tags\"]].sample(1000),\n",
    "    directory = \"Entrenamiento-maquina//Imagenes1/valid\",\n",
    "    x_col = \"photo_id\",\n",
    "    y_col = \"tags\",\n",
    "    batch_size = 32,\n",
    "    class_mode = \"categorical\",\n",
    "    shuffle=True,\n",
    "    color_mode = \"rgb\",\n",
    "    classes = keys,\n",
    "    target_size = (400,500))\n",
    "\n",
    "test_generator = datagen.flow_from_dataframe(\n",
    "    dataframe = data_test[[\"photo_id\",\"tags\"]].sample(1000),\n",
    "    directory = \"Entrenamiento-maquina//Imagenes1/test\",\n",
    "    x_col = \"photo_id\",\n",
    "    y_col = \"tags\",\n",
    "    batch_size = 32,\n",
    "    class_mode = None,\n",
    "    shuffle=True,\n",
    "    color_mode = \"rgb\",\n",
    "    target_size = (400,500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.core import Flatten\n",
    "from keras.layers.core import Dropout\n",
    "from keras.layers.core import Dense\n",
    "from keras import backend as K\n",
    "\n",
    "EPOCHS = 5\n",
    "INIT_LR = 1e-3\n",
    "BS = 100\n",
    "IMAGE_DIMS = (400,500,3)\n",
    "\n",
    "model = Sequential()\n",
    "inputShape = (400,500,3)\n",
    "chanDim = -1\n",
    "if K.image_data_format() == \"channels_first\":\n",
    "\t\t\tinputShape = (depth, height, width)\n",
    "\t\t\tchanDim = 1\n",
    "# CONV => RELU => POOL\n",
    "model.add(Conv2D(32, (3, 3), padding=\"same\",input_shape=inputShape))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization(axis=chanDim))\n",
    "model.add(MaxPooling2D(pool_size=(3, 3)))\n",
    "model.add(Dropout(0.25))\n",
    "# (CONV => RELU) * 2 => POOL\n",
    "model.add(Conv2D(64, (3, 3), padding=\"same\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization(axis=chanDim))\n",
    "model.add(Conv2D(64, (3, 3), padding=\"same\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization(axis=chanDim))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "# (CONV => RELU) * 2 => POOL\n",
    "model.add(Conv2D(128, (3, 3), padding=\"same\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization(axis=chanDim))\n",
    "model.add(Conv2D(128, (3, 3), padding=\"same\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization(axis=chanDim))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "# first (and only) set of FC => RELU layers\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1000))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "# softmax classifier\n",
    "model.add(Dense(983))\n",
    "model.add(Activation('sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    from keras.optimizers import Adam\n",
    "    opt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=opt,metrics=[\"accuracy\"])\n",
    "    # Corremos el modelo =========================================================#\n",
    "    STEP_SIZE_TRAIN=train_generator.n//train_generator.batch_size\n",
    "    STEP_SIZE_VALID=valid_generator.n//valid_generator.batch_size\n",
    "    STEP_SIZE_TEST=test_generator.n//test_generator.batch_size\n",
    "history = model.fit_generator(generator=train_generator,\n",
    "                    steps_per_epoch=STEP_SIZE_TRAIN,\n",
    "                    validation_data=valid_generator,\n",
    "                    validation_steps=STEP_SIZE_VALID,\n",
    "                    epochs=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficas de validaciones ====================================================#\n",
    "import matplotlib.pylab as plt\n",
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize']= 10,5\n",
    "fig, axs = plt.subplots(1, 2)\n",
    "axs[0].plot(history.history['loss'][1:], label='train')\n",
    "axs[0].plot(history.history['val_loss'][1:], label='test')\n",
    "axs[0].set_title('Loss')\n",
    "axs[0].legend()\n",
    "axs[1].plot(history.history['accuracy'][1:], label='train')\n",
    "axs[1].plot(history.history['val_accuracy'][1:], label='test')\n",
    "axs[1].set_title('Accuracy')\n",
    "axs[1].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import keras\n",
    "test_dir = 'Proyecto-Mineria-de-Datos/Imagenes/test'\n",
    "classes = train_generator.class_indices\n",
    "decode_classes = {v: k for k, v in classes.items()}\n",
    "decode_classes\n",
    "filenames_full = []\n",
    "for dirpath, dirnames, filenames in os.walk(test_dir):\n",
    "    for filename in filenames:\n",
    "        filenames_full.append(os.path.join(dirpath, filename))\n",
    "filenames_full = filenames_full[350:360]\n",
    "rows = (len(filenames_full) - 1) // 4 + 1\n",
    "plt.figure(figsize=(15, 5 * rows))\n",
    "for index, filename_full in enumerate(filenames_full):\n",
    "    plt.subplot(rows, 4, index + 1)\n",
    "    test_image = keras.preprocessing.image.load_img(filename_full, target_size=(400,500))\n",
    "    test_input = keras.preprocessing.image.img_to_array(test_image) * (1. / 255)\n",
    "    test_input = np.expand_dims(test_input, axis=0)\n",
    "    plt.imshow(test_image)\n",
    "    plt.axis('off')\n",
    "    prediction = model.predict(test_input)\n",
    "    prediction = prediction.reshape(983,)\n",
    "    print(prediction[np.argmax(prediction)])\n",
    "    type_name = decode_classes[np.argmax(prediction)]\n",
    "    plt.title(type_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
