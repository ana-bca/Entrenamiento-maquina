{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from skimage import io\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from spacy.lang.en import English\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer()\n",
    "nlp = English()\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import regularizers, optimizers\n",
    "from skimage.transform import rescale, resize, downscale_local_mean\n",
    "# Funciones ==================================================================#\n",
    "# Lectura de tablas ==========================================================#\n",
    "data_photo_F = pd.read_csv('Datos_recortados.csv', engine='python',sep=\",\")\n",
    "data_train = pd.read_csv('Imagenes1/data_train.csv', engine='python',sep=\"\\t\")\n",
    "data_test = pd.read_csv('Imagenes1/data_test.csv', engine='python',sep=\"\\t\")\n",
    "data_valid = pd.read_csv('Imagenes1/data_valid.csv', engine='python',sep=\"\\t\")\n",
    "\n",
    "Ngrams_1 = pd.read_csv('palabra30.csv', engine='python',sep=\";\")\n",
    "Ngrams_1=Ngrams_1[:10]\n",
    "#Ngrams_2 = pd.read_csv('prueba1.csv',sep=\",\")\n",
    "# Funciones ==================================================================#\n",
    "#N2 = Ngrams_2.compuesta.values\n",
    "def TUPT(x):\n",
    "    x_N1 = np.array(NGRAM(x,1))\n",
    "    x_N1 = x_N1.reshape(x_N1.shape[0],)\n",
    "    #x_N2 = pd.DataFrame(np.array(NGRAM(data_F.caption[0],2)))\n",
    "    #x_N2 = x_N2.agg(' '.join, axis=1).values\n",
    "    #x_N2 = np.array([x for x in x_N2 if x in N2])\n",
    "    #x_T = list(np.append(x_N1.astype(str), x_N2.astype(str)))\n",
    "    x_T = list(set(x_N1.astype(str)))\n",
    "    return x_T\n",
    "def NGRAM(x,n):\n",
    "    token=nltk.word_tokenize(str(x))\n",
    "    bigrams=ngrams(token,n)\n",
    "    return list(bigrams)\n",
    "# Codigo =====================================================================#\n",
    "data_photo_F = shuffle(data_photo_F)\n",
    "data_photo_F = data_photo_F[['photo_id','caption',]]\n",
    "data_photo_F.photo_id = data_photo_F.photo_id  + '.jpg'\n",
    "# Separacion de las imagenes en entrenamiento, test y validacion\n",
    "data_train, data_test = train_test_split(data_photo_F,test_size=0.2,random_state=1000)\n",
    "data_train, data_valid = train_test_split(data_train,test_size=20/80,random_state=1000)\n",
    "csv_train = \"Imagenes1/data_train.csv\"\n",
    "csv_test = \"Imagenes1/data_test.csv\"\n",
    "csv_valid = \"Imagenes1/data_valid.csv\"\n",
    "data_train.to_csv(csv_train , index=False,sep=\"\\t\")\n",
    "data_test.to_csv(csv_test , index=False,sep=\"\\t\")\n",
    "data_valid.to_csv(csv_valid , index=False,sep=\"\\t\")\n",
    "# Creamos los directorios para las imagenes\n",
    "for i in tqdm.tqdm(data_test.photo_id.values):\n",
    "    Imagen = io.imread('/home/ana/Documentos/maquinita/photos/' + i)\n",
    "    image_resized = resize(Imagen, (400,500),anti_aliasing=True)\n",
    "    local = 'Imagenes1/test/' + i\n",
    "    io.imsave(local,image_resized)\n",
    "for i in tqdm.tqdm(data_train.photo_id.values):\n",
    "    Imagen = io.imread('/home/ana/Documentos/maquinita/photos/' + i)\n",
    "    image_resized = resize(Imagen, (400,500),anti_aliasing=True)\n",
    "    local = 'Imagenes1/train/' + i\n",
    "    io.imsave(local,image_resized)\n",
    "for i in tqdm.tqdm(data_valid.photo_id.values):\n",
    "    Imagen = io.imread('/home/ana/Documentos/maquinita/photos/' + i)\n",
    "    image_resized = resize(Imagen, (400,500),anti_aliasing=True)\n",
    "    local = 'Imagenes1/valid/' + i\n",
    "    io.imsave(local,image_resized)\n",
    "# Generamos las clases en los dataframe\n",
    "    data_train, data_test = train_test_split(data_photo_F,test_size=0.2,random_state=1000)\n",
    "data_train, data_valid = train_test_split(data_train,test_size=20/80,random_state=1000)\n",
    "csv_train = \"Imagenes1/data_train.csv\"\n",
    "csv_test = \"Imagenes1/data_test.csv\"\n",
    "csv_valid = \"Imagenes1/data_valid.csv\"\n",
    "data_train.to_csv(csv_train , index=False,sep=\"\\t\")\n",
    "data_test.to_csv(csv_test , index=False,sep=\"\\t\")\n",
    "data_valid.to_csv(csv_valid , index=False,sep=\"\\t\")\n",
    "nltk.download('punkt')\n",
    "data_train = data_train.assign(tags = np.array(data_train.caption.map(lambda p: TUPT(p))))\n",
    "data_test = data_test.assign(tags = np.array(data_test.caption.map(lambda p: TUPT(p))))\n",
    "data_valid = data_valid.assign(tags = np.array(data_valid.caption.map(lambda p: TUPT(p))))\n",
    "# Cree un ImageDataGenerator con flow_from_dataframe"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
